# ğŸ¦ Real-Time Social Media Sentiment Analysis

Ce projet est une pipeline Big Data complÃ¨te capable d'ingÃ©rer, traiter et visualiser des sentiments (Positif / NÃ©gatif / Neutre) sur des flux de donnÃ©es de rÃ©seaux sociaux en temps rÃ©el.

---

##  Architecture

Le projet suit une architecture de streaming (type Lambda simplifiÃ©e) :

1. **Ingestion** : script Python simulant un flux Twitter via `Faker` (simulation de lâ€™API Twitter).
2. **Messaging** : **Apache Kafka** sert de tampon (buffer) haute performance pour transporter le flux.
3. **Processing** : **Spark Structured Streaming** lit Kafka, nettoie la donnÃ©e et applique un modÃ¨le NLP (`TextBlob`) pour calculer le sentiment.
4. **Storage** : **MongoDB** stocke les rÃ©sultats traitÃ©s.
5. **Visualization** : **Streamlit** + **Plotly** affichent les KPIs et l'Ã©volution temporelle en direct.

---

##  PrÃ©requis

- **Docker** & Docker Compose installÃ©s et fonctionnels
- **Python 3.8+**
- **Java 11** (requis pour Spark)
- Les dÃ©pendances Python listÃ©es dans `requirements.txt`

###  Note importante pour Windows

Le projet inclut un dossier `hadoop/bin` contenant `winutils.exe` et `hadoop.dll`.  
Ces fichiers sont indispensables pour faire tourner Spark sur Windows sans erreur.  
Le script `spark_processor.py` configure automatiquement les variables d'environnement pour utiliser ce dossier.

---

##  Installation

1. Cloner le dÃ©pÃ´t :

```bash
git clone https://github.com/Tedsuno/Sentiment_Analysis.git
cd Projet_BigData
```

2. Installer les dÃ©pendances Python :

```bash
pip install -r requirements.txt
```

---

## â–¶ DÃ©marrage 

Pour faire tourner tout le projet (Kafka + Spark + TextBlob + MongoDB + Dashboard Streamlit), il faut lancer plusieurs scripts **en parallÃ¨le** dans diffÃ©rents terminaux.

Voici les **4 Ã©tapes** Ã  suivre dans lâ€™ordre :

---

###  Ã‰tape 1 : Lancer lâ€™infrastructure (Terminal 1)

Dans un **Terminal 1**, placez-vous Ã  la racine du projet puis lancez :

```bash
docker-compose up -d
```

Cette commande dÃ©marre les services suivants :

- Zookeeper  
- Kafka  
- MongoDB  

Les conteneurs tournent en arriÃ¨re-plan et doivent rester actifs pendant toute la durÃ©e du projet.

Vous pouvez vÃ©rifier avec :

```bash
docker ps
```

---

###  Ã‰tape 2 : Lancer le traitement temps rÃ©el Spark + TextBlob (Terminal 1)

Toujours dans le **mÃªme Terminal 1**, une fois les conteneurs dÃ©marrÃ©s, lancez le job Spark Streaming :

```bash
python spark_processor.py
```

Ce script :

- lit en continu le topic Kafka `twitter_stream`
- applique le modÃ¨le NLP `TextBlob` pour calculer un sentiment (positif / NÃ©gatif / Neutre)
- Ã©crit les messages enrichis dans MongoDB (`twitter_db.sentiments`)

> Important : laissez ce terminal ouvert, sinon le traitement temps rÃ©el sâ€™arrÃªte.

---

###  Ã‰tape 3 : Lancer le producer Kafka (Terminal 3)

Ouvrez un **Terminal 2**, placez-vous dans le dossier du projet, puis lancez :

```bash
python producer.py
```

Ce script :

- gÃ©nÃ¨re de faux tweets (texte, auteur, topic, timestamp) via `Faker`
- envoie environ **un message par seconde** dans le topic Kafka `twitter_stream`

Vous verrez dÃ©filer des lignes du type :

```text
EnvoyÃ© : {'text': '...', 'author': '...', 'topic': 'tech', 'timestamp': 173...}
```

> Ce terminal doit aussi rester ouvert pour que le flux de donnÃ©es continue.

---

###  Ã‰tape 4 : Lancer le dashboard Streamlit (Terminal 3)

Ouvrez un **Terminal 3**, placez-vous dans le dossier du projet, puis lancez :

```bash
streamlit run dashboard.py
```

Cela ouvre lâ€™interface Streamlit (en gÃ©nÃ©ral sur :  
`http://localhost:8501`).

Le dashboard :

- lit les donnÃ©es dans MongoDB
- affiche les pourcentages de sentiments
- montre plusieurs graphiques :
  - rÃ©partition globale des sentiments
  - rÃ©partition par topic
  - sÃ©ries temporelles
  - heatmap topic Ã— sentiment
- liste les derniers messages dans un tableau qui se met Ã  jour en temps rÃ©el

> Tant que Spark (Terminal 1) et le producer (Terminal 2) tournent, le dashboard se met Ã  jour automatiquement.


##  ArrÃªt propre du systÃ¨me

1. Dans les Terminaux 1, 2 et 3, stoppez les scripts Python avec `Ctrl + C`.
2. Dans nâ€™importe quel terminal, arrÃªtez les conteneurs Docker :

```bash
docker-compose down
```

Tous les services (Kafka, Zookeeper, MongoDB) sont alors arrÃªtÃ©s.

---
