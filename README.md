# üê¶ Real-Time Social Media Sentiment Analysis

Ce projet est une pipeline Big Data compl√®te capable d'ing√©rer, traiter et visualiser des sentiments (Positif/N√©gatif/Neutre) sur des flux de donn√©es de r√©seaux sociaux en temps r√©el.

## üèóÔ∏è Architecture

Le projet suit une architecture Lambda simplifi√©e pour le streaming :

1.  **Ingestion :** Script Python simulant un flux Twitter via `Faker` (Simule l'API Twitter).
2.  **Messaging :** **Apache Kafka** sert de tampon (buffer) haute performance.
3.  **Processing :** **Spark Structured Streaming** lit Kafka, nettoie la donn√©e et applique un mod√®le NLP (`TextBlob`).
4.  **Storage :** **MongoDB** stocke les r√©sultats trait√©s.
5.  **Visualization :** **Streamlit** + **Plotly** affichent les KPIs et l'√©volution temporelle en direct.

---

## üöÄ Pr√©requis

* **Docker** & Docker Compose (install√©s et lanc√©s).
* **Python 3.8+**.
* **Java 11** (Requis pour Spark).

## üì¶ Installation

1.  Cloner le d√©p√¥t :
    ```bash
    git clone <VOTRE_LIEN_GIT>
    cd Projet_BigData
    ```

2.  Installer les d√©pendances Python :
    ```bash
    pip install -r requirements.txt
    ```

3.  **Note pour Windows :**
    Le projet inclut un dossier `hadoop/bin` avec `winutils.exe` et `hadoop.dll` n√©cessaires pour faire tourner Spark sur Windows sans erreur. Le script `spark_processor.py` configure automatiquement les variables d'environnement pour utiliser ce dossier.

---

## ‚ñ∂Ô∏è D√©marrage (Guide pas √† pas)

Il est important de lancer les services dans cet ordre pr√©cis en ouvrant **3 terminaux diff√©rents**.

### √âtape 1 : L'Infrastructure (Terminal 1)
Lancez les conteneurs (Zookeeper, Kafka, MongoDB) :
```bash
docker-compose up -d